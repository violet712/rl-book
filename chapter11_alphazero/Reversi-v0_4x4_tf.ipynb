{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 翻转棋 Reversi-v0\n",
    "$4 \\times 4$ 棋盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boardgame2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\37103\\我的云端硬盘\\rl-book\\chapter11_alphazero\\Reversi-v0_4x4_tf.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/37103/%E6%88%91%E7%9A%84%E4%BA%91%E7%AB%AF%E7%A1%AC%E7%9B%98/rl-book/chapter11_alphazero/Reversi-v0_4x4_tf.ipynb#ch0000001?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/37103/%E6%88%91%E7%9A%84%E4%BA%91%E7%AB%AF%E7%A1%AC%E7%9B%98/rl-book/chapter11_alphazero/Reversi-v0_4x4_tf.ipynb#ch0000001?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/37103/%E6%88%91%E7%9A%84%E4%BA%91%E7%AB%AF%E7%A1%AC%E7%9B%98/rl-book/chapter11_alphazero/Reversi-v0_4x4_tf.ipynb#ch0000001?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mboardgame2\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/37103/%E6%88%91%E7%9A%84%E4%BA%91%E7%AB%AF%E7%A1%AC%E7%9B%98/rl-book/chapter11_alphazero/Reversi-v0_4x4_tf.ipynb#ch0000001?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mboardgame2\u001b[39;00m \u001b[39mimport\u001b[39;00m BLACK, WHITE\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/37103/%E6%88%91%E7%9A%84%E4%BA%91%E7%AB%AF%E7%A1%AC%E7%9B%98/rl-book/chapter11_alphazero/Reversi-v0_4x4_tf.ipynb#ch0000001?line=15'>16</a>\u001b[0m logging\u001b[39m.\u001b[39mbasicConfig(stream\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstdout, level\u001b[39m=\u001b[39mlogging\u001b[39m.\u001b[39mDEBUG,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/37103/%E6%88%91%E7%9A%84%E4%BA%91%E7%AB%AF%E7%A1%AC%E7%9B%98/rl-book/chapter11_alphazero/Reversi-v0_4x4_tf.ipynb#ch0000001?line=16'>17</a>\u001b[0m         \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%(asctime)s\u001b[39;00m\u001b[39m [\u001b[39m\u001b[39m%(levelname)s\u001b[39;00m\u001b[39m] \u001b[39m\u001b[39m%(message)s\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'boardgame2'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import math\n",
    "import itertools\n",
    "import collections\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import boardgame2\n",
    "from boardgame2 import BLACK, WHITE\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境\n",
    "环境在 boardgame2 里实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Reversi-v0', board_shape=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(x, filters, kernel_sizes=3, strides=1, activations='relu',\n",
    "            regularizer=keras.regularizers.l2(1e-4)):\n",
    "    shortcut = x\n",
    "    for i, filte in enumerate(filters):\n",
    "        kernel_size = kernel_sizes if isinstance(kernel_sizes, int) \\\n",
    "                else kernel_sizes[i]\n",
    "        stride = strides if isinstance(strides, int) else strides[i]\n",
    "        activation = activations if isinstance(activations, str) \\\n",
    "                else activations[i]\n",
    "        z = keras.layers.Conv2D(filte, kernel_size, strides=stride,\n",
    "                padding='same', kernel_regularizer=regularizer,\n",
    "                bias_regularizer=regularizer)(x)\n",
    "        y = keras.layers.BatchNormalization()(z)\n",
    "        if i == len(filters) - 1:\n",
    "            y = keras.layers.Add()([shortcut, y])\n",
    "        x = keras.layers.Activation(activation)(y)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroAgent:\n",
    "    def __init__(self, env, batches=1, batch_size=4096,\n",
    "            kwargs={}, load=None, sim_count=800,\n",
    "            c_init=1.25, c_base=19652., prior_exploration_fraction=0.25):\n",
    "        self.env = env\n",
    "        self.board = np.zeros_like(env.board)\n",
    "        self.batches = batches\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.net = self.build_network(**kwargs)\n",
    "        self.reset_mcts()\n",
    "        self.sim_count = sim_count # MCTS 次数\n",
    "        self.c_init = c_init # PUCT 系数\n",
    "        self.c_base = c_base # PUCT 系数\n",
    "        self.prior_exploration_fraction = prior_exploration_fraction\n",
    "    \n",
    "    \n",
    "    def build_network(self, conv_filters, residual_filters, policy_filters,\n",
    "            learning_rate=0.001, regularizer=keras.regularizers.l2(1e-4)):\n",
    "        # 公共部分\n",
    "        inputs = keras.Input(shape=self.board.shape)\n",
    "        x = keras.layers.Reshape(self.board.shape + (1,))(inputs)\n",
    "        for conv_filter in conv_filters:\n",
    "            z = keras.layers.Conv2D(conv_filter, 3, padding='same',\n",
    "                    kernel_regularizer=regularizer,\n",
    "                    bias_regularizer=regularizer)(x)\n",
    "            y = keras.layers.BatchNormalization()(z)\n",
    "            x = keras.layers.ReLU()(y)\n",
    "        for residual_filter in residual_filters:\n",
    "            x = residual(x, filters=residual_filter, regularizer=regularizer)\n",
    "        intermediates = x\n",
    "        \n",
    "        # 概率部分\n",
    "        for policy_filter in policy_filters:\n",
    "            z = keras.layers.Conv2D(policy_filter, 3, padding='same',\n",
    "                    kernel_regularizer=regularizer,\n",
    "                    bias_regularizer=regularizer)(x)\n",
    "            y = keras.layers.BatchNormalization()(z)\n",
    "            x = keras.layers.ReLU()(y)\n",
    "        logits = keras.layers.Conv2D(1, 3, padding='same',\n",
    "                kernel_regularizer=regularizer, bias_regularizer=regularizer)(x)\n",
    "        flattens = keras.layers.Flatten()(logits)\n",
    "        softmaxs = keras.layers.Softmax()(flattens)\n",
    "        probs = keras.layers.Reshape(self.board.shape)(softmaxs)\n",
    "        \n",
    "        # 价值部分\n",
    "        z = keras.layers.Conv2D(1, 3, padding='same',\n",
    "                kernel_regularizer=regularizer,\n",
    "                bias_regularizer=regularizer)(intermediates)\n",
    "        y = keras.layers.BatchNormalization()(z)\n",
    "        x = keras.layers.ReLU()(y)\n",
    "        flattens = keras.layers.Flatten()(x)\n",
    "        vs = keras.layers.Dense(1, activation=keras.activations.tanh,\n",
    "                kernel_regularizer=regularizer,\n",
    "                bias_regularizer=regularizer)(flattens)\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=[probs, vs])\n",
    "\n",
    "        def categorical_crossentropy_2d(y_true, y_pred):\n",
    "            labels = tf.reshape(y_true, [-1, self.board.size])\n",
    "            preds = tf.reshape(y_pred, [-1, self.board.size])\n",
    "            return keras.losses.categorical_crossentropy(labels, preds)\n",
    "\n",
    "        loss = [categorical_crossentropy_2d, keras.losses.MSE]\n",
    "        optimizer = keras.optimizers.Adam(learning_rate)\n",
    "        model.compile(loss=loss, optimizer=optimizer)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def reset_mcts(self):\n",
    "        def zero_board_factory(): # 用于构造 default_dict\n",
    "            return np.zeros_like(self.board, dtype=float)\n",
    "        self.q = collections.defaultdict(zero_board_factory)\n",
    "            # q值估计: board -> board\n",
    "        self.count = collections.defaultdict(zero_board_factory)\n",
    "            # q值计数: board -> board\n",
    "        self.policy = {} # 策略: board -> board\n",
    "        self.valid = {} # 有效位置: board -> board\n",
    "        self.winner = {} # 赢家: board -> None or int\n",
    "    \n",
    "        \n",
    "    def decide(self, observation, greedy=False, return_prob=False):\n",
    "        # 计算策略\n",
    "        board, player = observation\n",
    "        canonical_board = player * board\n",
    "        s = boardgame2.strfboard(canonical_board)\n",
    "        while self.count[s].sum() < self.sim_count: # 多次 MCTS 搜索\n",
    "            self.search(canonical_board, prior_noise=True)\n",
    "        prob = self.count[s] / self.count[s].sum()\n",
    "\n",
    "        # 采样\n",
    "        location_index = np.random.choice(prob.size, p=prob.reshape(-1))\n",
    "        location = np.unravel_index(location_index, prob.shape)\n",
    "        if return_prob:\n",
    "            return location, prob\n",
    "        return location\n",
    "    \n",
    "    \n",
    "    def learn(self, dfs):\n",
    "        df = pd.concat(dfs).reset_index(drop=True)\n",
    "        for batch in range(self.batches):\n",
    "            indices = np.random.choice(len(df), size=self.batch_size)\n",
    "            players, boards, probs, winners = (np.stack(\n",
    "                    df.loc[indices, field]) for field in df.columns)\n",
    "            canonical_boards = players[:, np.newaxis, np.newaxis] * boards\n",
    "            vs = (players * winners)[:, np.newaxis]\n",
    "            self.net.fit(canonical_boards, [probs, vs], verbose=0) # 训练\n",
    "        self.reset_mcts()\n",
    "    \n",
    "    \n",
    "    def search(self, board, prior_noise=False): # MCTS 搜索\n",
    "        s = boardgame2.strfboard(board)\n",
    "        \n",
    "        if s not in self.winner:\n",
    "            self.winner[s] = self.env.get_winner((board, BLACK)) # 计算赢家\n",
    "        if self.winner[s] is not None: # 赢家确定的情况\n",
    "            return self.winner[s]\n",
    "        \n",
    "        if s not in self.policy: # 未计算过策略的叶子节点\n",
    "            pis, vs = self.net.predict(board[np.newaxis])\n",
    "            pi, v = pis[0], vs[0]\n",
    "            valid = self.env.get_valid((board, BLACK))\n",
    "            masked_pi = pi * valid\n",
    "            total_masked_pi = np.sum(masked_pi)\n",
    "            if total_masked_pi <= 0: # 所有的有效动作都没有概率，偶尔可能发生\n",
    "                masked_pi = valid # workaround\n",
    "                total_masked_pi = np.sum(masked_pi)\n",
    "            self.policy[s] = masked_pi / total_masked_pi\n",
    "            self.valid[s] = valid\n",
    "            return v\n",
    "        \n",
    "        # PUCT 上界计算\n",
    "        count_sum = self.count[s].sum()\n",
    "        coef = (self.c_init + np.log1p((1 + count_sum) / self.c_base)) * \\\n",
    "                math.sqrt(count_sum) / (1. + self.count[s])\n",
    "        if prior_noise: # 先验噪声\n",
    "            alpha = 1. / self.valid[s].sum()\n",
    "            noise = np.random.gamma(alpha, 1., board.shape)\n",
    "            noise *= self.valid[s]\n",
    "            noise /= noise.sum()\n",
    "            prior = (1. - self.prior_exploration_fraction) * \\\n",
    "                    self.policy[s] + \\\n",
    "                    self.prior_exploration_fraction * noise\n",
    "        else:\n",
    "            prior = self.policy[s]\n",
    "        ub = np.where(self.valid[s], self.q[s] + coef * prior, np.nan)\n",
    "        location_index = np.nanargmax(ub)\n",
    "        location = np.unravel_index(location_index, board.shape)\n",
    "        \n",
    "        (next_board, next_player), _, _, _ = self.env.next_step(\n",
    "                (board, BLACK), np.array(location))\n",
    "        next_canonical_board = next_player * next_board\n",
    "        next_v = self.search(next_canonical_board) # 递归搜索\n",
    "        v = next_player * next_v\n",
    "        \n",
    "        self.count[s][location] += 1\n",
    "        self.q[s][location] += (v - self.q[s][location]) / \\\n",
    "                self.count[s][location]\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 智能体和环境交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_play(env, agent, return_trajectory=False, verbose=False):\n",
    "    if return_trajectory:\n",
    "        trajectory = []\n",
    "    observation = env.reset()\n",
    "    for step in itertools.count():\n",
    "        board, player = observation\n",
    "        action, prob = agent.decide(observation, return_prob=True)\n",
    "        if verbose:\n",
    "            print(boardgame2.strfboard(board))\n",
    "            logging.info('第 {} 步：玩家 {}, 动作 {}'.format(step, player,\n",
    "                    action))\n",
    "        observation, winner, done, _ = env.step(action)\n",
    "        if return_trajectory:\n",
    "            trajectory.append((player, board, prob))\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(boardgame2.strfboard(observation[0]))\n",
    "                logging.info('赢家 {}'.format(winner))\n",
    "            break\n",
    "    if return_trajectory:\n",
    "        df_trajectory = pd.DataFrame(trajectory,\n",
    "                columns=['player', 'board', 'prob'])\n",
    "        df_trajectory['winner'] = winner\n",
    "        return df_trajectory\n",
    "    else:\n",
    "        return winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Canceled future for execute_request message before replies were done",
     "output_type": "error",
     "traceback": [
      "Error: Canceled future for execute_request message before replies were done",
      "at t.KernelShellFutureHandler.dispose (c:\\Users\\37103\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1204175)",
      "at c:\\Users\\37103\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1223227",
      "at Map.forEach (<anonymous>)",
      "at v._clearKernelState (c:\\Users\\37103\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1223212)",
      "at v.dispose (c:\\Users\\37103\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1216694)",
      "at c:\\Users\\37103\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:533674",
      "at t.swallowExceptions (c:\\Users\\37103\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:913059)",
      "at dispose (c:\\Users\\37103\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:533652)",
      "at t.RawSession.dispose (c:\\Users\\37103\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:537330)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (node:internal/process/task_queues:96:5)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AlphaZero 参数，可用来求解比较大型的问题（如五子棋）\n",
    "\"\"\"\n",
    "train_iterations = 700000 # 训练迭代次数\n",
    "train_episodes_per_iteration = 5000 # 每次迭代自我对弈回合数\n",
    "batches = 10 # 每回合进行几次批学习\n",
    "batch_size = 4096 # 批学习的批大小\n",
    "sim_count = 800 # MCTS需要的计数\n",
    "net_kwargs = {}\n",
    "net_kwargs['conv_filters'] = [256,]\n",
    "net_kwargs['residual_filters'] = [[256, 256],] * 19\n",
    "net_kwargs['policy_filters'] = [256,]\n",
    "\n",
    "\"\"\"\n",
    "小规模参数，用来初步求解比较小的问题（如井字棋）\n",
    "\"\"\"\n",
    "train_iterations = 100\n",
    "train_episodes_per_iteration = 100\n",
    "batches = 2\n",
    "batch_size = 64\n",
    "sim_count = 200\n",
    "net_kwargs = {}\n",
    "net_kwargs['conv_filters'] = [256,]\n",
    "net_kwargs['residual_filters'] = [[256, 256],]\n",
    "net_kwargs['policy_filters'] = [256,]\n",
    "\n",
    "agent = AlphaZeroAgent(env=env, kwargs=net_kwargs, sim_count=sim_count,\n",
    "        batches=batches, batch_size=batch_size)\n",
    "\n",
    "\n",
    "for iteration in range(train_iterations):\n",
    "    # 自我对弈\n",
    "    dfs_trajectory = []\n",
    "    for episode in range(train_episodes_per_iteration):\n",
    "        df_trajectory = self_play(env, agent,\n",
    "                return_trajectory=True, verbose=False)\n",
    "        logging.info('训练 {} 回合 {}: 收集到 {} 条经验'.format(\n",
    "                iteration, episode, len(df_trajectory)))\n",
    "        dfs_trajectory.append(df_trajectory)\n",
    "    \n",
    "    # 利用经验进行学习\n",
    "    agent.learn(dfs_trajectory)\n",
    "    logging.info('训练 {}: 学习完成'.format(iteration))\n",
    "    \n",
    "    # 演示训练结果\n",
    "    self_play(env, agent, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演示训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++\n",
      "+ox+\n",
      "+xo+\n",
      "++++\n",
      "2019-01-01 14:18:40,029 [INFO] 第 0 步：玩家 1, 动作 (1, 3)\n",
      "++++\n",
      "+ooo\n",
      "+xo+\n",
      "++++\n",
      "2019-01-01 14:18:40,029 [INFO] 第 1 步：玩家 -1, 动作 (0, 3)\n",
      "+++x\n",
      "+oxo\n",
      "+xo+\n",
      "++++\n",
      "2019-01-01 14:18:40,029 [INFO] 第 2 步：玩家 1, 动作 (0, 2)\n",
      "++ox\n",
      "+ooo\n",
      "+xo+\n",
      "++++\n",
      "2019-01-01 14:18:40,038 [INFO] 第 3 步：玩家 -1, 动作 (2, 3)\n",
      "++ox\n",
      "+oox\n",
      "+xxx\n",
      "++++\n",
      "2019-01-01 14:18:40,038 [INFO] 第 4 步：玩家 1, 动作 (3, 0)\n",
      "++ox\n",
      "+oox\n",
      "+oxx\n",
      "o+++\n",
      "2019-01-01 14:18:40,682 [INFO] 第 5 步：玩家 -1, 动作 (0, 1)\n",
      "+xxx\n",
      "+oxx\n",
      "+oxx\n",
      "o+++\n",
      "2019-01-01 14:18:40,757 [INFO] 第 6 步：玩家 1, 动作 (3, 3)\n",
      "+xxx\n",
      "+oxx\n",
      "+oox\n",
      "o++o\n",
      "2019-01-01 14:18:40,772 [INFO] 第 7 步：玩家 -1, 动作 (1, 0)\n",
      "+xxx\n",
      "xxxx\n",
      "+oox\n",
      "o++o\n",
      "2019-01-01 14:18:40,797 [INFO] 第 8 步：玩家 1, 动作 (0, 0)\n",
      "oxxx\n",
      "xoxx\n",
      "+oox\n",
      "o++o\n",
      "2019-01-01 14:18:40,807 [INFO] 第 9 步：玩家 -1, 动作 (2, 0)\n",
      "oxxx\n",
      "xxxx\n",
      "xxxx\n",
      "o++o\n",
      "2019-01-01 14:18:40,811 [INFO] 赢家 -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_play(env, agent, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06005ae5b55f54af46b1c5628efe94820d1dbffeaac4339668000bd55ec6c011"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
