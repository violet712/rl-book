{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 寻路问题 CliffWalking-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\37103\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ale_py\\roms\\__init__.py:89: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\n",
      "  ROMS = resolve_roms()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import scipy\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "观测空间 = Discrete(48)\n",
      "动作空间 = Discrete(4)\n",
      "状态数量 = 48, 动作数量 = 4\n",
      "地图大小 = (4, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\37103\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CliffWalking-v0')\n",
    "env.seed(0)\n",
    "print('观测空间 = {}'.format(env.observation_space))\n",
    "print('动作空间 = {}'.format(env.action_space))\n",
    "print('状态数量 = {}, 动作数量 = {}'.format(env.nS, env.nA))\n",
    "print('地图大小 = {}'.format(env.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行一回合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_once(env, policy):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        loc = np.unravel_index(state, env.shape)\n",
    "        print('状态 = {}, 位置 = {}'.format(state, loc), end=' ')\n",
    "        action = np.random.choice(env.nA, p=policy[state])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        print('动作 = {}, 奖励 = {}'.format(action, reward))\n",
    "        total_reward += reward\n",
    "        ###\n",
    "        env.render()\n",
    "        ###\n",
    "        if done:\n",
    "            ###\n",
    "            env.close()\n",
    "            ###\n",
    "            break\n",
    "        state = next_state\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用最优策略运行一回合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.ones(env.shape, dtype=int)\n",
    "actions[-1, :] = 0\n",
    "actions[:, -1] = 2\n",
    "optimal_policy = np.eye(4)[actions.reshape(-1)] # reshape(-1) 将数组转换为一维数组 \n",
    "                                                # eye()[] 数组转换为one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态 = 36, 位置 = (3, 0) 动作 = 0, 奖励 = -1\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "状态 = 24, 位置 = (2, 0) 动作 = 1, 奖励 = -1\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "状态 = 25, 位置 = (2, 1) 动作 = 1, 奖励 = -1\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  x  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "状态 = 26, 位置 = (2, 2) 动作 = 1, 奖励 = -1\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  x  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "状态 = 27, 位置 = (2, 3) 动作 = 1, 奖励 = -1\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  x  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "状态 = 28, 位置 = (2, 4) 动作 = 1, 奖励 = -1\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  x  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "状态 = 29, 位置 = (2, 5) 动作 = 1, 奖励 = -1\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  x  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "状态 = 30, 位置 = (2, 6) 动作 = 1, 奖励 = -1\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  x  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "状态 = 31, 位置 = (2, 7) 动作 = 1, 奖励 = -1\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  x  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "状态 = 32, 位置 = (2, 8) 动作 = 1, 奖励 = -1\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  x  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "状态 = 33, 位置 = (2, 9) 动作 = 1, 奖励 = -1\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  x  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "状态 = 34, 位置 = (2, 10) 动作 = 1, 奖励 = -1\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  x\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "状态 = 35, 位置 = (2, 11) 动作 = 2, 奖励 = -1\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  x\n",
      "\n",
      "回合奖励 = -13\n"
     ]
    }
   ],
   "source": [
    "total_reward = play_once(env, optimal_policy)\n",
    "print('回合奖励 = {}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 求解 Bellman 期望方程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bellman(env, policy, gamma=1.):\n",
    "    a, b = np.eye(env.nS), np.zeros((env.nS))\n",
    "    for state in range(env.nS - 1):\n",
    "        for action in range(env.nA):\n",
    "            pi = policy[state][action]\n",
    "            for p, next_state, reward, done in env.P[state][action]:\n",
    "                a[state, next_state] -= (pi * gamma * p)\n",
    "                b[state] += (pi * reward * p)\n",
    "    v = np.linalg.solve(a, b)\n",
    "    q = np.zeros((env.nS, env.nA))\n",
    "    for state in range(env.nS - 1):\n",
    "        for action in range(env.nA):\n",
    "            for p, next_state, reward, done in env.P[state][action]:\n",
    "                q[state][action] += ((reward + gamma * v[next_state]) * p)\n",
    "    return v, q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估随机策略的价值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态价值 = [-71259.73993046 -71237.78971176 -71194.84815307 -71140.80283084\n",
      " -71061.70657407 -70938.9109303  -68108.48102393 -66319.10665149\n",
      " -64517.35282333 -61466.53107191 -57841.39233559 -49099.24383729\n",
      " -71264.96522919 -71247.53092065 -71217.52824196 -71120.56871128\n",
      " -71054.59876056 -70900.38265543 -69089.0248754  -67348.61705426\n",
      " -65077.28575314 -59299.02626942 -54477.04027155 -43408.92846114\n",
      " -71343.97588504 -71365.86170718 -71356.29488211 -71310.51896579\n",
      " -71248.19189085 -70916.43626142 -70432.29357881 -69519.4952358\n",
      " -69113.6008844  -66843.01499868 -60740.99702848 -33361.65231114\n",
      " -71417.8714599  -71463.16859933 -71447.66816433 -71480.47316827\n",
      " -71514.26925748 -71213.20010573 -71233.56329316 -71021.31099414\n",
      " -70840.15827719 -69680.13842414 -65008.3404254       0.        ]\n",
      "动作价值 = [[-7.12607399e+04 -7.12387897e+04 -7.12659652e+04 -7.12607399e+04]\n",
      " [-7.12387897e+04 -7.11958482e+04 -7.12485309e+04 -7.12607399e+04]\n",
      " [-7.11958482e+04 -7.11418028e+04 -7.12185282e+04 -7.12387897e+04]\n",
      " [-7.11418028e+04 -7.10627066e+04 -7.11215687e+04 -7.11958482e+04]\n",
      " [-7.10627066e+04 -7.09399109e+04 -7.10555988e+04 -7.11418028e+04]\n",
      " [-7.09399109e+04 -6.81094810e+04 -7.09013827e+04 -7.10627066e+04]\n",
      " [-6.81094810e+04 -6.63201067e+04 -6.90900249e+04 -7.09399109e+04]\n",
      " [-6.63201067e+04 -6.45183528e+04 -6.73496171e+04 -6.81094810e+04]\n",
      " [-6.45183528e+04 -6.14675311e+04 -6.50782858e+04 -6.63201067e+04]\n",
      " [-6.14675311e+04 -5.78423923e+04 -5.93000263e+04 -6.45183528e+04]\n",
      " [-5.78423923e+04 -4.91002438e+04 -5.44780403e+04 -6.14675311e+04]\n",
      " [-4.91002438e+04 -4.91002438e+04 -4.34099285e+04 -5.78423923e+04]\n",
      " [-7.12607399e+04 -7.12485309e+04 -7.13449759e+04 -7.12659652e+04]\n",
      " [-7.12387897e+04 -7.12185282e+04 -7.13668617e+04 -7.12659652e+04]\n",
      " [-7.11958482e+04 -7.11215687e+04 -7.13572949e+04 -7.12485309e+04]\n",
      " [-7.11418028e+04 -7.10555988e+04 -7.13115190e+04 -7.12185282e+04]\n",
      " [-7.10627066e+04 -7.09013827e+04 -7.12491919e+04 -7.11215687e+04]\n",
      " [-7.09399109e+04 -6.90900249e+04 -7.09174363e+04 -7.10555988e+04]\n",
      " [-6.81094810e+04 -6.73496171e+04 -7.04332936e+04 -7.09013827e+04]\n",
      " [-6.63201067e+04 -6.50782858e+04 -6.95204952e+04 -6.90900249e+04]\n",
      " [-6.45183528e+04 -5.93000263e+04 -6.91146009e+04 -6.73496171e+04]\n",
      " [-6.14675311e+04 -5.44780403e+04 -6.68440150e+04 -6.50782858e+04]\n",
      " [-5.78423923e+04 -4.34099285e+04 -6.07419970e+04 -5.93000263e+04]\n",
      " [-4.91002438e+04 -4.34099285e+04 -3.33626523e+04 -5.44780403e+04]\n",
      " [-7.12659652e+04 -7.13668617e+04 -7.14188715e+04 -7.13449759e+04]\n",
      " [-7.12485309e+04 -7.13572949e+04 -7.15178715e+04 -7.13449759e+04]\n",
      " [-7.12185282e+04 -7.13115190e+04 -7.15178715e+04 -7.13668617e+04]\n",
      " [-7.11215687e+04 -7.12491919e+04 -7.15178715e+04 -7.13572949e+04]\n",
      " [-7.10555988e+04 -7.09174363e+04 -7.15178715e+04 -7.13115190e+04]\n",
      " [-7.09013827e+04 -7.04332936e+04 -7.15178715e+04 -7.12491919e+04]\n",
      " [-6.90900249e+04 -6.95204952e+04 -7.15178715e+04 -7.09174363e+04]\n",
      " [-6.73496171e+04 -6.91146009e+04 -7.15178715e+04 -7.04332936e+04]\n",
      " [-6.50782858e+04 -6.68440150e+04 -7.15178715e+04 -6.95204952e+04]\n",
      " [-5.93000263e+04 -6.07419970e+04 -7.15178715e+04 -6.91146009e+04]\n",
      " [-5.44780403e+04 -3.33626523e+04 -7.15178715e+04 -6.68440150e+04]\n",
      " [-4.34099285e+04 -3.33626523e+04 -1.00000000e+00 -6.07419970e+04]\n",
      " [-7.13449759e+04 -7.15178715e+04 -7.14188715e+04 -7.14188715e+04]\n",
      " [-7.13668617e+04 -7.15178715e+04 -7.15178715e+04 -7.14188715e+04]\n",
      " [-7.13572949e+04 -7.15178715e+04 -7.15178715e+04 -7.15178715e+04]\n",
      " [-7.13115190e+04 -7.15178715e+04 -7.15178715e+04 -7.15178715e+04]\n",
      " [-7.12491919e+04 -7.15178715e+04 -7.15178715e+04 -7.15178715e+04]\n",
      " [-7.09174363e+04 -7.15178715e+04 -7.15178715e+04 -7.15178715e+04]\n",
      " [-7.04332936e+04 -7.15178715e+04 -7.15178715e+04 -7.15178715e+04]\n",
      " [-6.95204952e+04 -7.15178715e+04 -7.15178715e+04 -7.15178715e+04]\n",
      " [-6.91146009e+04 -7.15178715e+04 -7.15178715e+04 -7.15178715e+04]\n",
      " [-6.68440150e+04 -7.15178715e+04 -7.15178715e+04 -7.15178715e+04]\n",
      " [-6.07419970e+04 -1.00000000e+00 -7.15178715e+04 -7.15178715e+04]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "policy = np.random.uniform(size=(env.nS, env.nA))\n",
    "policy = policy / np.sum(policy, axis=1)[:, np.newaxis]\n",
    "\n",
    "state_values, action_values = evaluate_bellman(env, policy)\n",
    "print('状态价值 = {}'.format(state_values))\n",
    "print('动作价值 = {}'.format(action_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估最优策略的价值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优状态价值 = [-14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3. -13. -12.\n",
      " -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2. -12. -11. -10.  -9.\n",
      "  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1. -13. -12. -11. -10.  -9.  -8.\n",
      "  -7.  -6.  -5.  -4.  -3.   0.]\n",
      "最优动作价值 = [[ -15.  -14.  -14.  -15.]\n",
      " [ -14.  -13.  -13.  -15.]\n",
      " [ -13.  -12.  -12.  -14.]\n",
      " [ -12.  -11.  -11.  -13.]\n",
      " [ -11.  -10.  -10.  -12.]\n",
      " [ -10.   -9.   -9.  -11.]\n",
      " [  -9.   -8.   -8.  -10.]\n",
      " [  -8.   -7.   -7.   -9.]\n",
      " [  -7.   -6.   -6.   -8.]\n",
      " [  -6.   -5.   -5.   -7.]\n",
      " [  -5.   -4.   -4.   -6.]\n",
      " [  -4.   -4.   -3.   -5.]\n",
      " [ -15.  -13.  -13.  -14.]\n",
      " [ -14.  -12.  -12.  -14.]\n",
      " [ -13.  -11.  -11.  -13.]\n",
      " [ -12.  -10.  -10.  -12.]\n",
      " [ -11.   -9.   -9.  -11.]\n",
      " [ -10.   -8.   -8.  -10.]\n",
      " [  -9.   -7.   -7.   -9.]\n",
      " [  -8.   -6.   -6.   -8.]\n",
      " [  -7.   -5.   -5.   -7.]\n",
      " [  -6.   -4.   -4.   -6.]\n",
      " [  -5.   -3.   -3.   -5.]\n",
      " [  -4.   -3.   -2.   -4.]\n",
      " [ -14.  -12.  -14.  -13.]\n",
      " [ -13.  -11. -113.  -13.]\n",
      " [ -12.  -10. -113.  -12.]\n",
      " [ -11.   -9. -113.  -11.]\n",
      " [ -10.   -8. -113.  -10.]\n",
      " [  -9.   -7. -113.   -9.]\n",
      " [  -8.   -6. -113.   -8.]\n",
      " [  -7.   -5. -113.   -7.]\n",
      " [  -6.   -4. -113.   -6.]\n",
      " [  -5.   -3. -113.   -5.]\n",
      " [  -4.   -2. -113.   -4.]\n",
      " [  -3.   -2.   -1.   -3.]\n",
      " [ -13. -113.  -14.  -14.]\n",
      " [ -12. -113. -113.  -14.]\n",
      " [ -11. -113. -113. -113.]\n",
      " [ -10. -113. -113. -113.]\n",
      " [  -9. -113. -113. -113.]\n",
      " [  -8. -113. -113. -113.]\n",
      " [  -7. -113. -113. -113.]\n",
      " [  -6. -113. -113. -113.]\n",
      " [  -5. -113. -113. -113.]\n",
      " [  -4. -113. -113. -113.]\n",
      " [  -3.   -1. -113. -113.]\n",
      " [   0.    0.    0.    0.]]\n"
     ]
    }
   ],
   "source": [
    "optimal_state_values, optimal_action_values = evaluate_bellman(env, optimal_policy)\n",
    "print('最优状态价值 = {}'.format(optimal_state_values))\n",
    "print('最优动作价值 = {}'.format(optimal_action_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 求解 Bellman 最优方程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_bellman(env, gamma=1.):\n",
    "    p = np.zeros((env.nS, env.nA, env.nS))\n",
    "    r = np.zeros((env.nS, env.nA))\n",
    "    for state in range(env.nS - 1):\n",
    "        for action in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                p[state, action, next_state] += prob\n",
    "                r[state, action] += (reward * prob)\n",
    "    c = np.ones(env.nS)\n",
    "    a_ub = gamma * p.reshape(-1, env.nS) - \\\n",
    "            np.repeat(np.eye(env.nS), env.nA, axis=0)\n",
    "    b_ub = -r.reshape(-1)\n",
    "    a_eq = np.zeros((0, env.nS))\n",
    "    b_eq = np.zeros(0)\n",
    "    bounds = [(None, None),] * env.nS\n",
    "    res = scipy.optimize.linprog(c, a_ub, b_ub, bounds=bounds,\n",
    "            method='interior-point')\n",
    "    v = res.x\n",
    "    q = r + gamma * np.dot(p, v)\n",
    "    return v, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy' has no attribute 'optimize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\vscodespace\\rl-book\\chapter02_mdp\\CliffWalking-v0.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/vscodespace/rl-book/chapter02_mdp/CliffWalking-v0.ipynb#ch0000018?line=0'>1</a>\u001b[0m optimal_state_values, optimal_action_values \u001b[39m=\u001b[39m optimal_bellman(env)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/vscodespace/rl-book/chapter02_mdp/CliffWalking-v0.ipynb#ch0000018?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m最优状态价值 = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(optimal_state_values))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/vscodespace/rl-book/chapter02_mdp/CliffWalking-v0.ipynb#ch0000018?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m最优动作价值 = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(optimal_action_values))\n",
      "\u001b[1;32md:\\vscodespace\\rl-book\\chapter02_mdp\\CliffWalking-v0.ipynb Cell 18'\u001b[0m in \u001b[0;36moptimal_bellman\u001b[1;34m(env, gamma)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/vscodespace/rl-book/chapter02_mdp/CliffWalking-v0.ipynb#ch0000017?line=13'>14</a>\u001b[0m b_eq \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/vscodespace/rl-book/chapter02_mdp/CliffWalking-v0.ipynb#ch0000017?line=14'>15</a>\u001b[0m bounds \u001b[39m=\u001b[39m [(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m),] \u001b[39m*\u001b[39m env\u001b[39m.\u001b[39mnS\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/vscodespace/rl-book/chapter02_mdp/CliffWalking-v0.ipynb#ch0000017?line=15'>16</a>\u001b[0m res \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49moptimize\u001b[39m.\u001b[39mlinprog(c, a_ub, b_ub, bounds\u001b[39m=\u001b[39mbounds,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/vscodespace/rl-book/chapter02_mdp/CliffWalking-v0.ipynb#ch0000017?line=16'>17</a>\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39minterior-point\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/vscodespace/rl-book/chapter02_mdp/CliffWalking-v0.ipynb#ch0000017?line=17'>18</a>\u001b[0m v \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mx\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/vscodespace/rl-book/chapter02_mdp/CliffWalking-v0.ipynb#ch0000017?line=18'>19</a>\u001b[0m q \u001b[39m=\u001b[39m r \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mdot(p, v)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'scipy' has no attribute 'optimize'"
     ]
    }
   ],
   "source": [
    "optimal_state_values, optimal_action_values = optimal_bellman(env)\n",
    "print('最优状态价值 = {}'.format(optimal_state_values))\n",
    "print('最优动作价值 = {}'.format(optimal_action_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优策略 = [2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "optimal_actions = optimal_action_values.argmax(axis=1)\n",
    "print('最优策略 = {}'.format(optimal_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  x  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  x  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for i in range(len(optimal_actions)):\n",
    "    env.step(optimal_actions[i])\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2638d182245c591c0c7650a148f9b4622aaf18b8cff9e48ecf203e602cb4653c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
